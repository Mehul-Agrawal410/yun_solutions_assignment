{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDV+FfZ14eaedKPXw9G5AB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehul-Agrawal410/yun_solutions_assignment/blob/main/solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kOlPRb2H2W8E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio(audio_path):\n",
        "    audio, _ = librosa.load(audio_path, sr=22050)  # Load audio file\n",
        "    return audio\n",
        "\n",
        "def extract_features(audio):\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=22050, n_mfcc=13)  # Extract Mel frequency cepstral coefficients\n",
        "    return mfccs\n",
        "\n",
        "def load_audio(file_path):\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    return audio, sr\n",
        "\n",
        "def extract_mfcc(audio, sr):\n",
        "    mfcc = librosa.feature.mfcc(audio, sr=sr, n_mfcc=13)\n",
        "    return mfcc"
      ],
      "metadata": {
        "id": "UDj-vLlY-SzY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I am using the Emo-DB dataset to train the model which will recogniz emotions , so you would need to get the [datset](https://www.kaggle.com/datasets/piyushagni5/berlin-database-of-emotional-speech-emodb?resource=download), download and extract it and put the path to the extracted repo here"
      ],
      "metadata": {
        "id": "ub3XDv4kjk5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "data_path = \"path_to_Emo-DB_dataset\"\n",
        "emotions = {\"W\": 0, \"L\": 1, \"E\": 2, \"A\": 3, \"F\": 4, \"T\": 5, \"N\": 6}  # Map emotions to numeric labels"
      ],
      "metadata": {
        "id": "hIL5dNKE8qeb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "for subdir, _, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".wav\"):\n",
        "            audio_path = os.path.join(subdir, file)\n",
        "            emotion_label = emotions[file[5]]  # Extract emotion label from file name\n",
        "            audio = preprocess_audio(audio_path)\n",
        "            features = extract_features(audio)  # Pass audio data to the extract_features function\n",
        "            X.append(features)\n",
        "            y.append(emotion_label)\n",
        "\n",
        "max_len = max(len(x) for x in X)\n",
        "X_padded = []\n",
        "for x in X:\n",
        "    if x.shape[1] < max_len:\n",
        "        x = np.pad(x, pad_width=((0, 0), (0, max_len - x.shape[1])), mode='constant')\n",
        "    elif x.shape[1] > max_len:\n",
        "        x = x[:, :max_len]\n",
        "    X_padded.append(x)\n",
        "X = np.array(X_padded)"
      ],
      "metadata": {
        "id": "cO_ZIR1-jZcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Unfortunately, I keep encountering error in the model.fit part here. It keeps giving ``ValueError: Found input variables with inconsistent numbers of samples: [4862, 374]`` or ``ValueError: zero-dimensional arrays cannot be concatenated`` or something else and I can't figure out how to resolve this, I even asked ChatGPT about it (it suggested concatenation) but it keeps circling in loops from 1 error to another and I am confident in my ability that I would figure this out if I had some more time"
      ],
      "metadata": {
        "id": "H9RvD7h0kbPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train_concatenated = np.vstack(X_train)\n",
        "y_train_concatenated = np.array(y_train)\n",
        "y_train_concatenated = y_train_concatenated.reshape(-1, 1)  # Reshape y_train_concatenated to match the number of samples\n",
        "model = SVC(kernel='rbf', gamma='scale')\n",
        "model.fit(X_train_concatenated, y_train_concatenated)\n",
        "\n",
        "y_val_pred = model.predict(np.concatenate(X_val))\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_report = classification_report(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "print(\"Validation Report:\\n\", val_report)\n",
        "\n",
        "y_test_pred = model.predict(np.concatenate(X_test))\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_report = classification_report(y_test, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Report:\\n\", test_report)"
      ],
      "metadata": {
        "id": "v1hiNqkki_KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The next part of the code is written *assuming* that the model works correctly"
      ],
      "metadata": {
        "id": "rekyoE8wlRX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "file_path = \"path_to_mp3_file.mp3\"\n",
        "audio, sr = load_audio(file_path)\n",
        "mfcc = extract_mfcc(audio, sr)"
      ],
      "metadata": {
        "id": "zg25H4k_jDHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_emotion(mfcc, model):\n",
        "    # Normalize MFCC features\n",
        "    mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
        "    # Reshape MFCC to match model input shape\n",
        "    mfcc = np.expand_dims(mfcc, axis=0)\n",
        "    mfcc = np.expand_dims(mfcc, axis=-1)\n",
        "    # Perform emotion classification\n",
        "    predictions = model.predict(mfcc)\n",
        "    # Map predictions to emotion labels\n",
        "    emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
        "    predicted_emotion = emotion_labels[np.argmax(predictions)]\n",
        "    return predicted_emotion\n",
        "def plot_emotional_changes(mfcc):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(mfcc, x_axis='time')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Emotional Changes')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1cOt5yIMjMzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_emotion = classify_emotion(mfcc, model)\n",
        "print(\"Predicted Emotion:\", predicted_emotion)\n",
        "plot_emotional_changes(mfcc)"
      ],
      "metadata": {
        "id": "GxfTUun5jSl-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}